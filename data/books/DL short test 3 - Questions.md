
## Short Test 3: Cnn, Rnn And Transformer, 7Min

Single-choice problem: All questions in this problem can be answered *independently*. Only one answer per question is correct. Please *circle* this answer on the problem sheet.

1. A CNN processes 2D input. What are the numbers of dimension of the kernel tensor and the output tensor of convolutional layers, respectively?

A: 3, 4 B: 4, 3 C: 3, 3 D: 2, 2 E: none of them 2. Let Np and N× be the number of model parameters and number of multiplications for one input sample in a convolutional layer, respectively. Which statement is correct?

A: Np = N× B: Np ≈ N× C: Np ≪ N× D: Np ≫ N×
3. Stride is used in a convolutional layer A: to increase the receptive field. B: to downsample the input.

C: for zero padding. D: to reduce the spatial size of the output.

4. Which statement is *wrong* about a recurrent neural network?

A: All layers have to be recurrent.

B: It has a longer memory than a feedforward network.

C: It contains feedback of neuron outputs.

D: Backpropagation through time is required. E: none of them 5. Which statement is *wrong* about a LSTM cell?

A: It is suitable for processing sequential data.

B: It is a type of recurrent neuron.

C: It uses gates to control input, memory state and output.

D: The gate signals are manually designed and static.

6. Which statement is *wrong* about a self-attention layer?

A: It looks at the similarity of all pairs of elements in the input sequence.

B: It captures long-range dependencies.

C: It returns an improved representation of the input sequence.

D: It is dependent on the position of the elements in the input sequence.

7. What is a vanilla transformer in deep learning?

A: It is a neural network for calculating the Fourier transform.

B: It is a neural network for seq2seq language processing relying on RNN.

C: It is a neural network for seq2seq language processing based on attention.

D: It is a neural network for computer vision based on attention.